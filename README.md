# Escrita Sincerta LLM

**Escrita Sincerta LLM** Ã© um projeto de cÃ³digo aberto para executar um Large Language Model (LLM) localmente, com uma interface web, agentes dedicados e memÃ³ria vetorial persistente. O projeto Ã© projetado para ser 100% offline, garantindo privacidade e controle total sobre seus dados.

## âœ¨ Features

- **ExecuÃ§Ã£o Local de LLM**: Rode modelos de linguagem de ponta em sua prÃ³pria mÃ¡quina, sem depender de serviÃ§os em nuvem.
- **Interface Web Intuitiva**: Interaja com o LLM atravÃ©s de uma interface web amigÃ¡vel, baseada no Open WebUI.
- **MemÃ³ria Vetorial Persistente**: Armazene e recupere informaÃ§Ãµes de conversas anteriores, permitindo que o LLM aprenda e evolua com o tempo.
- **Agentes Dedicados**: Utilize agentes especializados para tarefas especÃ­ficas, como desenvolvimento full-stack e escrita reflexiva.
- **Suporte a MÃºltiplos Modelos**: Alterne facilmente entre diferentes modelos de LLM, como `phi3`, `qwen2.5`, e `gpt-oss-20b`.
- **IngestÃ£o de Documentos**: EnriqueÃ§a o conhecimento do LLM com seus prÃ³prios documentos, atravÃ©s de um sistema de RAG (Retrieval-Augmented Generation).

## ğŸ› ï¸ Stack de Tecnologia

- **Ollama**: Orquestrador para execuÃ§Ã£o local de LLMs.
- **Open WebUI**: Interface web para interaÃ§Ã£o com o LLM.
- **Postgres + pgvector**: Banco de dados para armazenamento e busca de vetores.
- **FastAPI**: API Python para orquestraÃ§Ã£o dos serviÃ§os.
- **Docker**: ContainerizaÃ§Ã£o para facilitar a instalaÃ§Ã£o e o deploy.

## ğŸš€ ComeÃ§ando

Para comeÃ§ar a usar o Escrita Sincerta LLM, siga os passos abaixo:

1. **Clone o repositÃ³rio:**

   ```bash
   git clone https://github.com/seu-usuario/escrita-sincerta-llm.git
   cd escrita-sincerta-llm
   ```

2. **Configure o ambiente:**

   Copie o arquivo `.env.example` para `.env` e ajuste as variÃ¡veis de ambiente conforme necessÃ¡rio.

   ```bash
   cp .env.example .env
   ```

3. **Inicie os serviÃ§os:**

   Use o Docker Compose para iniciar todos os serviÃ§os.

   ```bash
   docker-compose up -d --build
   ```

4. **Baixe os modelos:**

   Execute o comando `make pull` para baixar os modelos de LLM prÃ©-configurados.

   ```bash
   make pull
   ```

## Uso

ApÃ³s a inicializaÃ§Ã£o, acesse a interface web em `http://localhost:3000`. VocÃª pode comeÃ§ar a conversar com o LLM, experimentar os diferentes agentes e fazer o upload de seus prÃ³prios documentos para ingestÃ£o.

## ğŸ“‚ Estrutura do Projeto

```
escrita-sincerta-llm/
 â”œâ”€ .env.example
 â”œâ”€ docker-compose.yml
 â”œâ”€ Makefile
 â”œâ”€ README.md
 â”œâ”€ data/
 â”‚   â”œâ”€ docs/
 â”‚   â””â”€ vectors/
 â”œâ”€ api/
 â”‚   â”œâ”€ app.py
 â”‚   â”œâ”€ agents/
 â”‚   â”œâ”€ tools/
 â”‚   â”œâ”€ prompts/
 â”‚   â””â”€ settings.py
 â””â”€ scripts/
     â”œâ”€ pull-models.sh
     â”œâ”€ dev.ps1
     â””â”€ dev.sh
```

## ğŸ”§ SoluÃ§Ã£o de Problemas

- **Interface nÃ£o responde**: Verifique se o serviÃ§o do Ollama estÃ¡ em execuÃ§Ã£o com `curl localhost:11434/api/tags`.
- **Modelo falha ao carregar**: Verifique os logs do Ollama e ajuste a quantizaÃ§Ã£o do modelo, se necessÃ¡rio.
- **Erro 500 na API**: Verifique os logs da API com `docker compose logs -f api`.

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se Ã  vontade para abrir uma issue ou enviar um pull request.

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a [MIT License](LICENSE).